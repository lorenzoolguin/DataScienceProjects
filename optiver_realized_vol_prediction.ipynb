{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-28T03:01:53.924588Z",
     "iopub.status.busy": "2021-09-28T03:01:53.924223Z",
     "iopub.status.idle": "2021-09-28T03:01:53.95461Z",
     "shell.execute_reply": "2021-09-28T03:01:53.953432Z",
     "shell.execute_reply.started": "2021-09-28T03:01:53.924493Z"
    }
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "MODE = 'TRAIN'\n",
    "#MODE = 'Inference'\n",
    "MODEL_DIR = '../input/optiver-lgb-and-te-baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:03.484954Z",
     "iopub.status.busy": "2021-09-28T03:02:03.484599Z",
     "iopub.status.idle": "2021-09-28T03:02:03.502571Z",
     "shell.execute_reply": "2021-09-28T03:02:03.501238Z",
     "shell.execute_reply.started": "2021-09-28T03:02:03.484917Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import pathlib\n",
    "from tqdm.auto import tqdm # Widget progress bar\n",
    "import json\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import requests as re\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta, FR\n",
    "\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from sklearn import model_selection\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "from matplotlib_venn import venn2, venn3\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "sns.set_context('talk')\n",
    "style.use('seaborn-colorblind')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "pd.get_option('display.max_columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:03.504658Z",
     "iopub.status.busy": "2021-09-28T03:02:03.504401Z",
     "iopub.status.idle": "2021-09-28T03:02:03.512135Z",
     "shell.execute_reply": "2021-09-28T03:02:03.511178Z",
     "shell.execute_reply.started": "2021-09-28T03:02:03.504629Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    INPUT_DIR = '../input/optiver-realized-volatility-prediction'\n",
    "    OUTPUT_DIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:04.30285Z",
     "iopub.status.busy": "2021-09-28T03:02:04.301896Z",
     "iopub.status.idle": "2021-09-28T03:02:04.311112Z",
     "shell.execute_reply": "2021-09-28T03:02:04.310326Z",
     "shell.execute_reply.started": "2021-09-28T03:02:04.302813Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_logger(log_file='train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter('%(message)s'))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter('%(message)s'))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "logger = init_logger(log_file=f'{CFG.OUTPUT_DIR}/baseline.log')\n",
    "logger.info(f'Start Logging...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:04.914782Z",
     "iopub.status.busy": "2021-09-28T03:02:04.914475Z",
     "iopub.status.idle": "2021-09-28T03:02:24.285027Z",
     "shell.execute_reply": "2021-09-28T03:02:24.284204Z",
     "shell.execute_reply.started": "2021-09-28T03:02:04.91475Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(CFG.INPUT_DIR, 'train.csv'))  # target, what you are trying to predict, first appears in train\n",
    "test = pd.read_csv(os.path.join(CFG.INPUT_DIR, 'test.csv'))\n",
    "ss = pd.read_csv(os.path.join(CFG.INPUT_DIR, 'sample_submission.csv'))\n",
    "book = pd.read_parquet(os.path.join(CFG.INPUT_DIR, 'book_train.parquet'))\n",
    "trade = pd.read_parquet(os.path.join(CFG.INPUT_DIR, 'trade_train.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:24.287106Z",
     "iopub.status.busy": "2021-09-28T03:02:24.286782Z",
     "iopub.status.idle": "2021-09-28T03:02:24.294703Z",
     "shell.execute_reply": "2021-09-28T03:02:24.29392Z",
     "shell.execute_reply.started": "2021-09-28T03:02:24.287051Z"
    }
   },
   "outputs": [],
   "source": [
    "train_book_stocks = os.listdir(os.path.join(CFG.INPUT_DIR, 'book_train.parquet'))\n",
    "\n",
    "if DEBUG:\n",
    "    logger.info('Debug mode: using 3 stocks only')\n",
    "    train_book_stocks = train_book_stocks[:3]\n",
    "    \n",
    "logger.info('{:,} train book stocks: {}'.format(len(train_book_stocks), train_book_stocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:24.296452Z",
     "iopub.status.busy": "2021-09-28T03:02:24.296148Z",
     "iopub.status.idle": "2021-09-28T03:02:24.315805Z",
     "shell.execute_reply": "2021-09-28T03:02:24.314741Z",
     "shell.execute_reply.started": "2021-09-28T03:02:24.296413Z"
    }
   },
   "outputs": [],
   "source": [
    "#load stock_id=0\n",
    "def load_book(stock_id, data_type='train'):\n",
    "    \"\"\"\n",
    "    load parquet book data for given stock_id\n",
    "    \"\"\"\n",
    "    #the following line imports the parquet book train data and selects the specified stock_id to be stored in book_df\n",
    "    book_df = pd.read_parquet(os.path.join(CFG.INPUT_DIR, f'book_{data_type}.parquet/stock_id={stock_id}')) \n",
    "    \n",
    "    book_df['stock_id'] = stock_id\n",
    "    book_df['stock_id'] = book_df['stock_id'].astype(np.int8)\n",
    "    \n",
    "    return book_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:24.318502Z",
     "iopub.status.busy": "2021-09-28T03:02:24.317947Z",
     "iopub.status.idle": "2021-09-28T03:02:24.330651Z",
     "shell.execute_reply": "2021-09-28T03:02:24.329727Z",
     "shell.execute_reply.started": "2021-09-28T03:02:24.318457Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_trade(stock_id=0, data_type='train'):\n",
    "    \"\"\"\n",
    "    load parquet trade data for given stock_id\n",
    "    \"\"\"\n",
    "    #the following line imports the parquet trade train data and selects the specified stock_id to be stored in trade_df\n",
    "    trade_df = pd.read_parquet(os.path.join(CFG.INPUT_DIR, f'trade_{data_type}.parquet/stock_id={stock_id}'))\n",
    "    trade_df['stock_id'] = stock_id\n",
    "    trade_df['stock_id'] = trade_df['stock_id'].astype(np.int8)\n",
    "    \n",
    "    return trade_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:24.332392Z",
     "iopub.status.busy": "2021-09-28T03:02:24.331923Z",
     "iopub.status.idle": "2021-09-28T03:02:24.347442Z",
     "shell.execute_reply": "2021-09-28T03:02:24.346593Z",
     "shell.execute_reply.started": "2021-09-28T03:02:24.332343Z"
    }
   },
   "outputs": [],
   "source": [
    "def fix_jsonerr(df):\n",
    "    # fix json column error for lightgbm\n",
    "    # isalnum returns true if all digits in string are alphanumeric\n",
    "    df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:24.349103Z",
     "iopub.status.busy": "2021-09-28T03:02:24.348783Z",
     "iopub.status.idle": "2021-09-28T03:02:24.359804Z",
     "shell.execute_reply": "2021-09-28T03:02:24.358947Z",
     "shell.execute_reply.started": "2021-09-28T03:02:24.349048Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:24.361458Z",
     "iopub.status.busy": "2021-09-28T03:02:24.36113Z",
     "iopub.status.idle": "2021-09-28T03:02:24.372304Z",
     "shell.execute_reply": "2021-09-28T03:02:24.37109Z",
     "shell.execute_reply.started": "2021-09-28T03:02:24.361421Z"
    }
   },
   "outputs": [],
   "source": [
    "def realized_volatility(series_log_return):\n",
    "    series_log_return = log_return(series_log_return)\n",
    "    return np.sqrt(np.sum(series_log_return ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:24.373859Z",
     "iopub.status.busy": "2021-09-28T03:02:24.373548Z",
     "iopub.status.idle": "2021-09-28T03:02:24.385182Z",
     "shell.execute_reply": "2021-09-28T03:02:24.384544Z",
     "shell.execute_reply.started": "2021-09-28T03:02:24.37382Z"
    }
   },
   "outputs": [],
   "source": [
    "def fe_row(book):\n",
    "    #Feature engineering (just volatility) for each row\n",
    "    \n",
    "    #volatility\n",
    "    for i in[1,2, ]:\n",
    "        #wap\n",
    "        book[f'book_wap{i}'] = (book[f'bid_price{i}'] * book[f'ask_size{i}'] + book[f'ask_price{i}'] * book[f'bid_size{i}']) / (book[f'bid_size{i}'] + book[f'ask_size{i}'])\n",
    "            \n",
    "    #mean wap\n",
    "    book['book_wap_mean'] = (book['book_wap1'] + book['book_wap2']) / 2\n",
    "    \n",
    "    #wap diff\n",
    "    book['book_wap_diff'] = book['book_wap1'] - book['book_wap2']\n",
    "    \n",
    "    #other orderbook features\n",
    "    book['book_price_spread'] = (book['ask_price1'] - book['bid_price1']) / (book['ask_price1'] + book['bid_price1'])\n",
    "    book['book_bid_spread'] = book['bid_price1'] - book['bid_price2']\n",
    "    book['book_ask_spread'] = book['ask_price1'] - book['ask_price2']\n",
    "    book['book_total_volume'] = book['ask_size1'] + book['ask_size2'] + book['bid_size1'] + book['bid_size2']\n",
    "    book['book_volume_imbalance'] = (book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2'])\n",
    "    \n",
    "    return book "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:24.387196Z",
     "iopub.status.busy": "2021-09-28T03:02:24.386541Z",
     "iopub.status.idle": "2021-09-28T03:02:24.398752Z",
     "shell.execute_reply": "2021-09-28T03:02:24.398057Z",
     "shell.execute_reply.started": "2021-09-28T03:02:24.387144Z"
    }
   },
   "outputs": [],
   "source": [
    "def fe_agg(book_df):\n",
    "    #feature engineering (aggregation by stock_id x time_id)\n",
    "    \n",
    "    # features\n",
    "    book_feats = book_df.columns[book_df.columns.str.startswith('book_')].values.tolist()\n",
    "    trade_feats = ['price', 'size', 'order_count', 'seconds_in_bucket']\n",
    "    \n",
    "    # agg trade features\n",
    "    trade_df = book_df.groupby(['time_id', 'stock_id'])[trade_feats].agg(['sum', 'mean', 'std', 'max', 'min']).reset_index()\n",
    "    \n",
    "    # agg volatility features\n",
    "    fe_df = book_df.groupby(['time_id', 'stock_id'])[book_feats].agg([realized_volatility]).reset_index()\n",
    "    fe_df.columns = [\" \".join(col).strip() for col in fe_df.columns.values]\n",
    "    \n",
    "    # merge\n",
    "    fe_df = fe_df.merge(trade_df, how='left', on=['time_id', 'stock_id'])\n",
    "    \n",
    "    return fe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:24.402253Z",
     "iopub.status.busy": "2021-09-28T03:02:24.40173Z",
     "iopub.status.idle": "2021-09-28T03:02:24.415649Z",
     "shell.execute_reply": "2021-09-28T03:02:24.414742Z",
     "shell.execute_reply.started": "2021-09-28T03:02:24.402205Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is where the previous functions come together\n",
    "def fe_all(book_df):\n",
    "    \"\"\"\n",
    "    perform feature engineerings\n",
    "    \"\"\"\n",
    "      \n",
    "    # row-wise feature engineering, ADDS THE EXTRA FEATURES TO THE DATAFRAME\n",
    "    book_df = fe_row(book_df)\n",
    "    \n",
    "    # feature engineering agg by stock_id x time_id, ADDS REALIZED VOLATILITY TO SOME OF THE FEATURES\n",
    "    fe_df = fe_agg(book_df)\n",
    "    \n",
    "    return fe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:24.417522Z",
     "iopub.status.busy": "2021-09-28T03:02:24.416545Z",
     "iopub.status.idle": "2021-09-28T03:02:24.429733Z",
     "shell.execute_reply": "2021-09-28T03:02:24.428942Z",
     "shell.execute_reply.started": "2021-09-28T03:02:24.417474Z"
    }
   },
   "outputs": [],
   "source": [
    "def book_fe_by_stock(stock_id=0):\n",
    "    \"\"\"\n",
    "    load orderbook and trade data for the given stock_id and merge\n",
    "    \n",
    "    \"\"\"\n",
    "    # load data\n",
    "    book_df = load_book(stock_id, 'train')\n",
    "    trade_df = load_trade(stock_id, 'train')\n",
    "    book_feats = book_df.columns.values.tolist()\n",
    "    \n",
    "    # merge\n",
    "    book_df = book_df.merge(trade_df, how='outer', on=['time_id', 'seconds_in_bucket', 'stock_id'])\n",
    "    \n",
    "    # sort by time\n",
    "    book_df = book_df.sort_values(by=['time_id', 'seconds_in_bucket'])\n",
    "    \n",
    "    # fillna for book_df\n",
    "    book_df[book_feats] = book_df[book_feats].fillna(method='ffill')\n",
    "    \n",
    "    # feature engineering\n",
    "    fe_df = fe_all(book_df)\n",
    "    return fe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:24.430862Z",
     "iopub.status.busy": "2021-09-28T03:02:24.430645Z",
     "iopub.status.idle": "2021-09-28T03:02:24.441459Z",
     "shell.execute_reply": "2021-09-28T03:02:24.440653Z",
     "shell.execute_reply.started": "2021-09-28T03:02:24.430837Z"
    }
   },
   "outputs": [],
   "source": [
    "def book_fe_by_stock_test(stock_id=0):\n",
    "    \"\"\"\n",
    "    same function but for the test\n",
    "    \n",
    "    \"\"\"\n",
    "    # load data\n",
    "    book_df = load_book(stock_id, 'test')\n",
    "    trade_df = load_trade(stock_id, 'test')\n",
    "    book_feats = book_df.columns.values.tolist()\n",
    "    \n",
    "    # merge\n",
    "    book_df = book_df.merge(\n",
    "        trade_df\n",
    "        , how='outer'\n",
    "        , on=['time_id', 'seconds_in_bucket', 'stock_id']\n",
    "    )\n",
    "    \n",
    "    # sort by time\n",
    "    book_df = book_df.sort_values(by=['time_id', 'seconds_in_bucket'])\n",
    "    \n",
    "    # fillna for book_df\n",
    "    book_df[book_feats] = book_df[book_feats].fillna(method='ffill')\n",
    "    \n",
    "    # feature engineering\n",
    "    fe_df = fe_all(book_df)\n",
    "    return fe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:24.443013Z",
     "iopub.status.busy": "2021-09-28T03:02:24.442377Z",
     "iopub.status.idle": "2021-09-28T03:02:24.459052Z",
     "shell.execute_reply": "2021-09-28T03:02:24.458351Z",
     "shell.execute_reply.started": "2021-09-28T03:02:24.44298Z"
    }
   },
   "outputs": [],
   "source": [
    "def book_fe_all(stock_ids, data_type='train'):\n",
    "    # feature engineering with multithread processing\n",
    "    \n",
    "    # feature engineering agg by stock_id x time_id\n",
    "    with Pool(cpu_count()) as p:\n",
    "        if data_type == 'train':\n",
    "            feature_dfs = list(tqdm(p.imap(book_fe_by_stock, stock_ids), total=len(stock_ids)))\n",
    "        elif data_type == 'test':\n",
    "            feature_dfs = list(tqdm(p.imap(book_fe_by_stock_test, stock_ids), total=len(stock_ids)))\n",
    "            \n",
    "    fe_df = pd.concat(feature_dfs)\n",
    "    \n",
    "    # feature engineering agg by stock_id\n",
    "    vol_feats = [f for f in fe_df.columns if ('realized' in f) & ('wap' in f)]\n",
    "    if data_type == 'train':\n",
    "        # agg\n",
    "        stock_df = fe_df.groupby('stock_id')[vol_feats].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "        \n",
    "        # fix column names\n",
    "        stock_df.columns = ['stock_id'] + [f'{f}_stock' for f in stock_df.columns.values.tolist()[1:]]\n",
    "        stock_df = fix_jsonerr(stock_df)\n",
    "        \n",
    "    # feature engineering agg by time_id\n",
    "    time_df = fe_df.groupby('time_id')[vol_feats].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "    time_df.columns = ['time_id'] + [f'{f}_time' for f in time_df.columns.values.tolist()[1:]]\n",
    "    \n",
    "    # merge\n",
    "    fe_df = fe_df.merge(time_df, how='left', on='time_id')\n",
    "    \n",
    "    # make sure to fix json error for lightgbm\n",
    "    fe_df = fix_jsonerr(fe_df)\n",
    "    \n",
    "    # out\n",
    "    if data_type == 'train':\n",
    "        return fe_df, stock_df\n",
    "    elif data_type == 'test':\n",
    "        return fe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:02:24.460434Z",
     "iopub.status.busy": "2021-09-28T03:02:24.460116Z",
     "iopub.status.idle": "2021-09-28T03:16:47.520513Z",
     "shell.execute_reply": "2021-09-28T03:16:47.519346Z",
     "shell.execute_reply.started": "2021-09-28T03:02:24.460405Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if MODE == 'TRAIN':\n",
    "    # all book data feature engineering\n",
    "    stock_ids = [int(i.split('=')[-1]) for i in train_book_stocks]\n",
    "    book_df, stock_df = book_fe_all(stock_ids, data_type='train')\n",
    "    \n",
    "    assert book_df['stock_id'].nunique() > 2\n",
    "    assert book_df['time_id'].nunique() > 2\n",
    "    \n",
    "    # save stock_df for the test\n",
    "    stock_df.to_pickle('train_stock_df.pkl')\n",
    "    logger.info('train stock df saved!')\n",
    "    \n",
    "    # merge \n",
    "    book_df = book_df.merge(stock_df, how='left', on='stock_id').merge(train, how='left', on=['stock_id', 'time_id']).replace([np.inf, -np.inf], np.nan).fillna(method='ffill')\n",
    "    \n",
    "    # make row_id\n",
    "    book_df['row_id'] = book_df['stock_id'].astype(str) + '-' + book_df['time_id'].astype(str)\n",
    "    \n",
    "    print(book_df.shape)\n",
    "    book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:16:47.522731Z",
     "iopub.status.busy": "2021-09-28T03:16:47.522362Z",
     "iopub.status.idle": "2021-09-28T03:16:47.559042Z",
     "shell.execute_reply": "2021-09-28T03:16:47.558217Z",
     "shell.execute_reply.started": "2021-09-28T03:16:47.52268Z"
    }
   },
   "outputs": [],
   "source": [
    "book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:16:47.560373Z",
     "iopub.status.busy": "2021-09-28T03:16:47.560166Z",
     "iopub.status.idle": "2021-09-28T03:16:48.311891Z",
     "shell.execute_reply": "2021-09-28T03:16:48.310478Z",
     "shell.execute_reply.started": "2021-09-28T03:16:47.560349Z"
    }
   },
   "outputs": [],
   "source": [
    "# test\n",
    "test_book_stocks = os.listdir(os.path.join(CFG.INPUT_DIR, 'book_test.parquet'))\n",
    "\n",
    "logger.info('{:,} test book stocks: {}'.format(len(test_book_stocks), test_book_stocks))\n",
    "\n",
    "# all book data feature engineering\n",
    "test_stocks_ids = [int(i.split('=')[-1]) for i in test_book_stocks]\n",
    "test_book_df = book_fe_all(test_stocks_ids, data_type='test')\n",
    "\n",
    "# load stock_df, if inference\n",
    "if MODE == 'INFERENCE':\n",
    "    stock_df = pd.read_pickle(f'{MODEL_DIR}/train_stock_df.pkl')\n",
    "    \n",
    "# merge\n",
    "test_book_df = test.merge(stock_df, how='left', on='stock_id').merge(test_book_df, how='left', on=['stock_id', 'time_id']).replace([np.inf, -np.inf], np.nan).fillna(method='ffill')\n",
    "\n",
    "# make row_id\n",
    "test_book_df['row_id'] = test_book_df['stock_id'].astype(str) + '-' + test_book_df['time_id'].astype(str)\n",
    "\n",
    "print(test_book_df.shape)\n",
    "test_book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:16:48.314662Z",
     "iopub.status.busy": "2021-09-28T03:16:48.313915Z",
     "iopub.status.idle": "2021-09-28T03:16:48.456827Z",
     "shell.execute_reply": "2021-09-28T03:16:48.45593Z",
     "shell.execute_reply.started": "2021-09-28T03:16:48.314609Z"
    }
   },
   "outputs": [],
   "source": [
    "target = 'target'\n",
    "drops = [target, 'row_id', 'time_id']\n",
    "features = [f for f in test_book_df.columns.values.tolist() if (f not in drops) & (test_book_df[f].isna().sum() == 0) & (book_df[f].isna().sum() == 0)]\n",
    "cats = ['stock_id']\n",
    "\n",
    "logger.info('{:,} features ({:,} categorical): {}'.format(len(features), len(cats), features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:16:48.4669Z",
     "iopub.status.busy": "2021-09-28T03:16:48.466478Z",
     "iopub.status.idle": "2021-09-28T03:16:48.483091Z",
     "shell.execute_reply": "2021-09-28T03:16:48.48234Z",
     "shell.execute_reply.started": "2021-09-28T03:16:48.46687Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluation metric\n",
    "def RMSPEMetric(XGBoost=False):\n",
    "    \n",
    "    def RMSPE(yhat, dtrain, XGBoost = XGBoost):\n",
    "        \n",
    "        y = dtrain.get_label()\n",
    "        elements = ((y - yhat) / y) ** 2\n",
    "        if XGBoost:\n",
    "            return 'RMSPE', float(np.sqrt(np.sum(elements) / len(y)))\n",
    "        else:\n",
    "            return 'RMSPE', float(np.sqrt(np.sum(elements) / len(y))), False\n",
    "        \n",
    "    return RMSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:16:48.484526Z",
     "iopub.status.busy": "2021-09-28T03:16:48.484316Z",
     "iopub.status.idle": "2021-09-28T03:16:48.49541Z",
     "shell.execute_reply": "2021-09-28T03:16:48.494823Z",
     "shell.execute_reply.started": "2021-09-28T03:16:48.4845Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_model(params, X_train, y_train, X_test, features=features, cats=[], era='stock_id', fold_type='kfold', n_fold = 5, seed=42):\n",
    "    \"\"\"\n",
    "    fit model with cv (cross validation)\n",
    "    \"\"\"\n",
    "    \n",
    "    models = []\n",
    "    oof_df = X_train[['time_id', 'stock_id', target]].copy()\n",
    "    oof_df['pred'] = np.nan\n",
    "    y_preds = np.zeros((len(X_test),))\n",
    "    \n",
    "    if fold_type == 'stratifiedshuffle':\n",
    "        cv = model_selecion.StratifiedShuffleSplit(n_splits=n_fold, random_state=seed)\n",
    "        kf = cv.split(X_train, X_train[era])\n",
    "    elif fold_type == 'kfold':\n",
    "        cv = model_selection.KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "        kf = cv.split(X_train, y_train)\n",
    "    \n",
    "    fi_df = pd.DataFrame()\n",
    "    fi_df['features'] = features\n",
    "    fi_df['importance'] = 0\n",
    "    \n",
    "    for fold_id, (train_index, valid_index) in tqdm(enumerate(kf)):\n",
    "        #split\n",
    "        X_tr = X_train.loc[train_index, features]\n",
    "        X_val = X_train.loc[valid_index, features]\n",
    "        y_tr = y_train.loc[train_index]\n",
    "        y_val = y_train.loc[valid_index]\n",
    "        \n",
    "        \n",
    "        # model, note inverse weighting please\n",
    "        train_set = lgb.Dataset(X_tr, y_tr, categorical_feature = cats, weight = 1/np.power(y_tr, 2))\n",
    "        val_set = lgb.Dataset(X_val, y_val, categorical_feature = cats, weight = 1/np.power(y_val, 2))\n",
    "        # model training\n",
    "        model = lgb.train(params, train_set, valid_sets = [train_set, val_set], feval=RMSPEMetric(), verbose_eval=250)\n",
    "        \n",
    "        # feature importance\n",
    "        fi_df[f'importance_fold{fold_id}'] = model.feature_importance(importance_type='gain')\n",
    "        fi_df['importance'] += fi_df[f'importance_fold{fold_id}'].values\n",
    "        \n",
    "        # save model\n",
    "        joblib.dump(model, f'model_fold{fold_id}.pkl')\n",
    "        logger.debug('model saved!')\n",
    "        \n",
    "        # predict\n",
    "        oof_df['pred'].iloc[valid_index] = model.predict(X_val)\n",
    "        y_pred = model.predict(X_test[features])\n",
    "        y_preds += y_pred / n_fold\n",
    "        models.append(model)\n",
    "        \n",
    "    return oof_df, y_preds, models, fi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:16:48.496852Z",
     "iopub.status.busy": "2021-09-28T03:16:48.496481Z",
     "iopub.status.idle": "2021-09-28T03:16:48.51323Z",
     "shell.execute_reply": "2021-09-28T03:16:48.512243Z",
     "shell.execute_reply.started": "2021-09-28T03:16:48.496825Z"
    }
   },
   "source": [
    "### Notes on hyperparamters: \n",
    "#### num_leaves, max_depth (between 3 and 12), min_data_in_leaf in hundreds or thousands for big datasets (specifies the minimum number of observations that fit the decision criteria in a leaf.)  n_estimators, learning_rate, max_bin (maybe), lambda_l1, lambda_l2, min_gain_to_split, bagging_fraction, feature_fraction, bagging_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:16:48.516423Z",
     "iopub.status.busy": "2021-09-28T03:16:48.516158Z",
     "iopub.status.idle": "2021-09-28T03:16:49.487423Z",
     "shell.execute_reply": "2021-09-28T03:16:49.486677Z",
     "shell.execute_reply.started": "2021-09-28T03:16:48.516392Z"
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial, features=features, X=book_df, y=book_df[target], cats=[]):\n",
    "    param_grid = {\"n_estimators\": trial.suggest_categorical(\"n_estimators\", [813]),\n",
    "        \"boosting\": trial.suggest_categorical(\"boosting\",['dart']),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.1, 0.2),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 511, 750),\n",
    "        #\"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 617, 800),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 300),\n",
    "        #\"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "        #\"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 0.75),\n",
    "        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1,7]),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.8, 0.95)\n",
    "    }\n",
    "    \n",
    "    cv = model_selection.KFold(n_splits=2, shuffle=True, random_state=46) #n_splits is reduced to 2 for time saving\n",
    "    #kf = cv.split(X_train, y_train)\n",
    "    cv_scores = np.empty(2) # from optuna blog\n",
    "    \n",
    "    # splits the data into groups, uses one as validation, trains models and gets results then repeats\n",
    "    for fold_id, (train_index, valid_index) in tqdm(enumerate(cv.split(X,y))): \n",
    "        #split\n",
    "        X_tr = X.loc[train_index, features]\n",
    "        X_val = X.loc[valid_index, features]\n",
    "        y_tr = y.loc[train_index]\n",
    "        y_val = y.loc[valid_index]\n",
    "        \n",
    "        \n",
    "        # model, note inverse weighting please\n",
    "        train_set = lgb.Dataset(X_tr, y_tr, categorical_feature = cats, weight = 1/np.power(y_tr, 2))\n",
    "        val_set = lgb.Dataset(X_val, y_val, categorical_feature = cats, weight = 1/np.power(y_val, 2))\n",
    "        # Note that train() will return a model from the best iteration.\n",
    "        model = lgb.train(param_grid, train_set, valid_sets = [train_set, val_set], feval=RMSPEMetric(), verbose_eval=250)\n",
    "        #model.fit(X_train=book_df,y_train = book_df[target],test_book_df,cats = cats)\n",
    "        \n",
    "        # predict\n",
    "        y_pred = model.predict(X_val[features])\n",
    "        y_true = y_val\n",
    "        return (np.sqrt(np.mean(np.square((y_true - y_pred)/y_true))))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T03:16:49.489155Z",
     "iopub.status.busy": "2021-09-28T03:16:49.488569Z"
    }
   },
   "outputs": [],
   "source": [
    "#optimization_function = partial(objective, X=x, y=y)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-28T02:38:10.186564Z",
     "iopub.status.idle": "2021-09-28T02:38:10.187122Z",
     "shell.execute_reply": "2021-09-28T02:38:10.186957Z",
     "shell.execute_reply.started": "2021-09-28T02:38:10.186937Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_model(params, X_train, y_train, X_test, features=features, cats=[], era='stock_id', fold_type='kfold', n_fold = 2, seed=42):\n",
    "    \"\"\"\n",
    "    fit model with cv (cross validation)\n",
    "    \"\"\"\n",
    "    \n",
    "    models = []\n",
    "    oof_df = X_train[['time_id', 'stock_id', target]].copy()\n",
    "    oof_df['pred'] = np.nan\n",
    "    y_preds = np.zeros((len(X_test),))\n",
    "    \n",
    "    if fold_type == 'stratifiedshuffle':\n",
    "        cv = model_selecion.StratifiedShuffleSplit(n_splits=n_fold, random_state=seed)\n",
    "        kf = cv.split(X_train, X_train[era])\n",
    "    elif fold_type == 'kfold':\n",
    "        cv = model_selection.KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "        kf = cv.split(X_train, y_train)\n",
    "    \n",
    "    fi_df = pd.DataFrame()\n",
    "    fi_df['features'] = features\n",
    "    fi_df['importance'] = 0\n",
    "    \n",
    "    for fold_id, (train_index, valid_index) in tqdm(enumerate(kf)):\n",
    "        #split\n",
    "        X_tr = X_train.loc[train_index, features]\n",
    "        X_val = X_train.loc[valid_index, features]\n",
    "        y_tr = y_train.loc[train_index]\n",
    "        y_val = y_train.loc[valid_index]\n",
    "        \n",
    "        \n",
    "        # model, note inverse weighting please\n",
    "        train_set = lgb.Dataset(X_tr, y_tr, categorical_feature = cats, weight = 1/np.power(y_tr, 2))\n",
    "        val_set = lgb.Dataset(X_val, y_val, categorical_feature = cats, weight = 1/np.power(y_val, 2))\n",
    "        # model training\n",
    "        model = lgb.train(params, train_set, valid_sets = [train_set, val_set], feval=RMSPEMetric(), verbose_eval=250)\n",
    "        \n",
    "        # feature importance\n",
    "        fi_df[f'importance_fold{fold_id}'] = model.feature_importance(importance_type='gain')\n",
    "        fi_df['importance'] += fi_df[f'importance_fold{fold_id}'].values\n",
    "        \n",
    "        # save model\n",
    "        joblib.dump(model, f'model_fold{fold_id}.pkl')\n",
    "        logger.debug('model saved!')\n",
    "        \n",
    "        # predict\n",
    "        oof_df['pred'].iloc[valid_index] = model.predict(X_val)\n",
    "        y_pred = model.predict(X_test[features])\n",
    "        y_preds += y_pred / n_fold\n",
    "        models.append(model)\n",
    "        \n",
    "    return oof_df, y_preds, models, fi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-28T02:38:10.190045Z",
     "iopub.status.idle": "2021-09-28T02:38:10.190704Z",
     "shell.execute_reply": "2021-09-28T02:38:10.190533Z",
     "shell.execute_reply.started": "2021-09-28T02:38:10.190513Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : 813,\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'dart',\n",
    "    'max_depth': -1,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 511,\n",
    "    'min_data_in_leaf': 617,\n",
    "    'max_bin': 70,\n",
    "    'subsample': 0.82,\n",
    "    'subsample_freq': 7,\n",
    "    'feature_fraction': 0.6,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1,\n",
    "    'min_gain_to_split': 0.009275035155177913,\n",
    "    'seed': 46,\n",
    "    'early_stopping_rounds': 75,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "if MODE == 'TRAIN':\n",
    "    oof_df, y_preds, models, fi_df = fit_model(params,\n",
    "                                              book_df,\n",
    "                                              book_df[target],\n",
    "                                              test_book_df,\n",
    "                                              features=features,\n",
    "                                              cats = cats,\n",
    "                                              era = None,\n",
    "                                              fold_type = 'kfold',\n",
    "                                              n_fold = 2,\n",
    "                                              seed = 46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance and Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-28T02:38:10.191846Z",
     "iopub.status.idle": "2021-09-28T02:38:10.192589Z",
     "shell.execute_reply": "2021-09-28T02:38:10.192421Z",
     "shell.execute_reply.started": "2021-09-28T02:38:10.192401Z"
    }
   },
   "outputs": [],
   "source": [
    "Feature Importance\n",
    "if MODE == 'TRAIN':\n",
    "    fi_df = fi_df.sort_values(by='importance', ascending=False)\n",
    "    fi_df.to_csv('feature_importance.csv', index=False)\n",
    "    \n",
    "    #fig, ax = plt.subplots(1, 1, figsize=(10, 40))\n",
    "    #sns.barplot(x='importance', y='features', data=fi_df.iloc[:30], ax=ax)\n",
    "    logger.info(fi_df[['features', 'importance']].iloc[:50].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-28T02:38:10.193628Z",
     "iopub.status.idle": "2021-09-28T02:38:10.193994Z",
     "shell.execute_reply": "2021-09-28T02:38:10.193823Z",
     "shell.execute_reply.started": "2021-09-28T02:38:10.193802Z"
    }
   },
   "outputs": [],
   "source": [
    "reduced_features = (fi_df['features'].loc[:35])\n",
    "reduced_features = reduced_features.to_list()\n",
    "reduced_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_features = ['book_wap1_realized_volatility',\n",
    " 'book_wap2_realized_volatility',\n",
    " 'book_wap_mean_realized_volatility',\n",
    " 'stock_id',\n",
    " '__book_wap2_realized_volatility____min___time',\n",
    " 'book_bid_spread_realized_volatility',\n",
    " '__book_wap_mean_realized_volatility____mean___time',\n",
    " '__book_wap1_realized_volatility____min___time',\n",
    " 'book_total_volume_realized_volatility',\n",
    " '__book_wap_mean_realized_volatility____min___time',\n",
    " '__book_wap2_realized_volatility____mean___time',\n",
    " '__price____std__',\n",
    " '__book_wap1_realized_volatility____mean___stock',\n",
    " '__book_wap1_realized_volatility____mean___time',\n",
    " '__book_wap_diff_realized_volatility____min___time',\n",
    " '__book_wap1_realized_volatility____max___time',\n",
    " '__book_wap_mean_realized_volatility____max___time',\n",
    " '__book_wap2_realized_volatility____max___time',\n",
    " '__price____sum__',\n",
    " '__seconds_in_bucket____sum__',\n",
    " '__seconds_in_bucket____mean__',\n",
    " '__order_count____sum__',\n",
    " 'book_wap_diff_realized_volatility',\n",
    " '__order_count____std__',\n",
    " '__book_wap_mean_realized_volatility____mean___stock',\n",
    " 'book_price_spread_realized_volatility',\n",
    " '__seconds_in_bucket____std__',\n",
    " '__book_wap1_realized_volatility____min___stock',\n",
    " '__price____min__',\n",
    " '__size____max__',\n",
    " '__order_count____mean__',\n",
    " '__price____max__',\n",
    " '__size____mean__',\n",
    " '__size____std__',\n",
    " '__size____min__',\n",
    " '__book_wap2_realized_volatility____mean___stock',\n",
    " '__price____mean__',\n",
    " '__order_count____max__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-28T02:38:10.195011Z",
     "iopub.status.idle": "2021-09-28T02:38:10.195306Z",
     "shell.execute_reply": "2021-09-28T02:38:10.195169Z",
     "shell.execute_reply.started": "2021-09-28T02:38:10.195155Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "params_1 = {\n",
    "    'n_estimators' : 813,\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'dart',\n",
    "    'max_depth': -1,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 511,\n",
    "    'min_data_in_leaf': 617,\n",
    "    'max_bin': 70,\n",
    "    'subsample': 0.82,\n",
    "    'subsample_freq': 7,\n",
    "    'feature_fraction': 0.6,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1,\n",
    "    #'min_gain_to_split': 0.009275035155177913,\n",
    "    'seed': 46,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "if MODE == 'TRAIN':\n",
    "    oof_df, y_preds, models, fi_df = fit_model(params_1,\n",
    "                                              book_df,\n",
    "                                              book_df[target],\n",
    "                                              test_book_df,\n",
    "                                              features=reduced_features,\n",
    "                                              cats = cats,\n",
    "                                              era = None,\n",
    "                                              fold_type = 'kfold',\n",
    "                                              n_fold = 3,\n",
    "                                              seed = 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-28T02:38:10.196726Z",
     "iopub.status.idle": "2021-09-28T02:38:10.197063Z",
     "shell.execute_reply": "2021-09-28T02:38:10.196913Z",
     "shell.execute_reply.started": "2021-09-28T02:38:10.196897Z"
    }
   },
   "outputs": [],
   "source": [
    "MODE == 'INFERENCE'\n",
    "if MODE == 'INFERENCE':\n",
    "    \"\"\"\n",
    "    used for inference kernel only\n",
    "    \"\"\"\n",
    "    y_preds = np.zeros(len(test_book_df))\n",
    "    files = glob.glob(f'{MODEL_DIR}/*model*.pkl')\n",
    "    assert len(files) > 0\n",
    "    for i, f in enumerate(files):\n",
    "        model = joblib.load(f)\n",
    "        y_preds += model.predict(test_book_df[features])\n",
    "    y_preds /= (i+1)\n",
    "    \n",
    "test_book_df[target] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-28T02:38:10.198131Z",
     "iopub.status.idle": "2021-09-28T02:38:10.198443Z",
     "shell.execute_reply": "2021-09-28T02:38:10.198304Z",
     "shell.execute_reply.started": "2021-09-28T02:38:10.198289Z"
    }
   },
   "outputs": [],
   "source": [
    "# test\n",
    "test_book_df[target] = y_preds\n",
    "\n",
    "# save the submit file\n",
    "sub = test_book_df[['row_id', target]]\n",
    "sub.to_csv('submission.csv',index = False)\n",
    "\n",
    "logger.info('submitted!')\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
